from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments, GPT2Tokenizer, GPT2Model
from datasets import load_from_disk
import torch
import yaml
import logging
from pathlib import Path

logger = logging.getLogger('LLM_TrainTokenize')
logger.setLevel(logging.ERROR)
error_handler = logging.StreamHandler()
error_handler = logging.FileHandler(Path('C:/Users/Vikram Pande/Side_Projects/Error_Logs/LLM_TrainTokenize_log.log'))
error_handler.setLevel(logging.ERROR)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
error_handler.setFormatter(formatter)
logger.addHandler(error_handler)

# Load the file paths and global variables from YAML config file
try:
    config_path = Path('C:/Users/Vikram Pande/Side_Projects/Generative_AI_LLM')

    with open(config_path / 'config.yml', 'r') as file:
        global_vars = yaml.safe_load(file)
except:
    logger.error(f'{config_path} YAML Configuration file path not found. Please check the storage path of the \'config.yml\' file and try again')

corpus_path = global_vars['saved_models_path']
pretrained_model = global_vars['pretrained_model']
training_log_path = global_vars['training_log_path']
model_output_path = global_vars['model_output_path']

# Use GPT-2 for the pretrained tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)
model = GPT2Model.from_pretrained(pretrained_model)

# Verify if GPU is being used and mount the Pre-trained Model and inputs to the GPU
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
print('Is GPU available?: ', device)
model.to(device)

# Load the tokenized custom dataset (tokenized job ads generated by the 'Job_Ad_Q&A_Train_Tokenize' module)
tokenized_jobads = load_from_disk(Path(corpus_path))

# train_dataset = TextDataset(tokenized_jobads, tokenizer, block_size=128)
# When using a Pretrained model (such as https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) that contain special tokens like [INST], [/INST], etc. set 'mlm': Masked Language Modelling to 'True'
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(per_device_train_batch_size=2, 
                                  num_train_epochs=3,
                                  logging_dir=training_log_path,
                                  output_dir=model_output_path)
